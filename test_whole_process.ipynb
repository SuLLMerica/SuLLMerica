{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for generating the challenge test answers\n",
    "To evaluate only the public test set, comment the `\"./Data/test/questions_new.txt\",` line in the first code section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ujson as json\n",
    "import re\n",
    "import os\n",
    "import traceback\n",
    "from Source.query import Query\n",
    "from Source.maneger_dataset import get_embeddings_by_labels\n",
    "from Source.generate_question import generate_questions_training\n",
    "from Source.enhancement_query import EnhancementQuery\n",
    "from Source.get_RAG_context import Get_RAG_Context\n",
    "from tqdm import tqdm\n",
    "import sqlite3\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "PUBLIC_TEST = False\n",
    "\n",
    "TEST_FILES=[\n",
    "    \"./Data/test/questions_new.txt\", \n",
    "    \"./Data/test/TeleQnA_testing1.txt\"\n",
    "] if not PUBLIC_TEST else [\"./Data/test/TeleQnA_testing1.txt\"]\n",
    "\n",
    "\n",
    "NUM_CLUSTERS = 18\n",
    "TOP_K_CLUSTERS = 8\n",
    "TOP_K_CHUNCKS = 5\n",
    "DATABASE_PATH=\"cluster_data_BisectingKMeans_18_250_chunksize.db\"\n",
    "TEST_FULL_DATASET = \"./Data/TeleQnA_Dataset_Full.txt\"\n",
    "TEST_DATA_WITH_RAG_PATH = \"./Data/intermediates/TeleQnA_Test_With_RAG_Context.json\"\n",
    "PATH_TERMS_FILE = \"./Data/TermsAndDefinitions/terms_definitions.json\"\n",
    "PATH_ABBREVIATIONS_FILE = \"./Data/TermsAndDefinitions/abbreviations_definitions.json\"\n",
    "MODEL_NAME = \"claudiomello/Phi-2-TeleQnA-Finetune-Final\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data and generating context (only run if not using our provided data with generated context)\n",
    "Only run the next two sections if you want to manually generate the RAG context, if not run the getting the provided RAG data section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create an empty dictionary\n",
    "test_data = {}\n",
    "\n",
    "# Read both files and concatenate data\n",
    "for file in TEST_FILES:\n",
    "    with open(file) as f:\n",
    "        test_data.update(json.load(f))\n",
    "\n",
    "if PUBLIC_TEST:\n",
    "    full_data = {}\n",
    "    with open(TEST_FULL_DATASET) as f:\n",
    "        full_data.update(json.load(f))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a class for enhancement\n",
    "enhacenment_query = EnhancementQuery(file_name_terms=PATH_TERMS_FILE, file_name_abbreviations=PATH_ABBREVIATIONS_FILE)\n",
    "\n",
    "# Create an array to store the data\n",
    "test_data_json = []\n",
    "\n",
    "# Iterate over the test data for adding the terms and abbreviations\n",
    "for question in test_data.keys():\n",
    "    question_id = int(question.split(\" \")[1])\n",
    "\n",
    "    if PUBLIC_TEST:\n",
    "        try:\n",
    "            pub_answer = full_data[question][\"answer\"]\n",
    "            pub_answer = int(pub_answer[7:8])\n",
    "        except:\n",
    "            pub_answer = -2\n",
    "            print(f\"Error in question {question} when trying to get the public answer\")\n",
    "\n",
    "    terms, abreviations = enhacenment_query.define_TA_question(test_data[question][\"question\"])\n",
    "\n",
    "\n",
    "    data = {\n",
    "        \"question\": test_data[question][\"question\"],\n",
    "        \"question_id\": question_id,\n",
    "        \"terms\": terms,\n",
    "        \"abbreviations\": abreviations,\n",
    "        \"answer\": pub_answer if PUBLIC_TEST else None\n",
    "    }\n",
    "    if \"option 1\" in test_data[question]:\n",
    "        data[\"option 1\"] = test_data[question][\"option 1\"]\n",
    "    if \"option 2\" in test_data[question]:\n",
    "        data[\"option 2\"] = test_data[question][\"option 2\"]\n",
    "    if \"option 3\" in test_data[question]:\n",
    "        data[\"option 3\"] = test_data[question][\"option 3\"]\n",
    "    if \"option 4\" in test_data[question]:\n",
    "        data[\"option 4\"] = test_data[question][\"option 4\"]\n",
    "    if \"option 5\" in test_data[question]:\n",
    "        data[\"option 5\"] = test_data[question][\"option 5\"]\n",
    "    test_data_json.append(data)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a progress bar\n",
    "pb = tqdm(\n",
    "    test_data_json,\n",
    "    total=len(test_data_json),\n",
    "    desc=\"Generating RAG Contexts\",\n",
    "    unit=\"question\",\n",
    ")\n",
    "\n",
    "# Create a list to store the questions\n",
    "test_data_json_with_context=[]\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(DATABASE_PATH)\n",
    "\n",
    "\n",
    "# Iterate over the questions for generating the RAG context\n",
    "for question_data in pb:\n",
    "\n",
    "\n",
    "    # Get the question\n",
    "    question = question_data[\"question\"]\n",
    "\n",
    "    # Get the options\n",
    "    options = {}\n",
    "    try:\n",
    "        option_1 = str(question_data[\"option 1\"])\n",
    "        if option_1 != \"nan\" and option_1 != \"\":\n",
    "            options[\"option 1\"] = option_1\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        option_2 = str(question_data[\"option 2\"])\n",
    "        if option_2 != \"nan\" and option_2 != \"\":\n",
    "            options[\"option 2\"] = option_2\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        option_3 = str(question_data[\"option 3\"])\n",
    "        if option_3 != \"nan\" and option_3 != \"\":\n",
    "            options[\"option 3\"] = option_3\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        option_4 = str(question_data[\"option 4\"])\n",
    "        if option_4 != \"nan\" and option_4 != \"\":\n",
    "            options[\"option 4\"] = option_4\n",
    "    except KeyError:\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        option_5 = str(question_data[\"option 5\"])\n",
    "        if option_5 != \"nan\" and option_5 != \"\":\n",
    "            options[\"option 5\"] = option_5\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Get the terms and abbreviations\n",
    "    terms = None\n",
    "    if str(question_data[\"terms\"]) != \"nan\" and question_data[\"terms\"] != \"\":\n",
    "        terms = question_data[\"terms\"]\n",
    "    \n",
    "    abbreviations = None\n",
    "    if str(question_data[\"abbreviations\"]) != \"nan\" and question_data[\"abbreviations\"] != \"\":\n",
    "        abbreviations = question_data[\"abbreviations\"]\n",
    "    \n",
    "    \n",
    "    # Generate the RAG context\n",
    "\n",
    "    try:\n",
    "        context = Get_RAG_Context(question, conn, NUM_CLUSTERS, TOP_K_CLUSTERS, TOP_K_CHUNCKS)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    test_data_json_with_context.append({\n",
    "        \"question\": question,\n",
    "        \"Question_ID\": question_data[\"question_id\"],\n",
    "        \"options\": options,\n",
    "        \"terms\": terms,\n",
    "        \"abbreviations\": abbreviations,\n",
    "        \"context\": context,\n",
    "        \"answer\": question_data[\"answer\"] if PUBLIC_TEST else None\n",
    "    })\n",
    "\n",
    "\n",
    "# Close the connection to the database\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the provided RAG data\n",
    "Run this section **only** if you skipped the gernerating step earlier, it will **overwrite** the previous generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEST_DATA_WITH_RAG_PATH, \"r\") as file:\n",
    "    test_data_json_with_context = json.load(file)\n",
    "\n",
    "if PUBLIC_TEST:\n",
    "    aux = []\n",
    "    for question in test_data_json_with_context:\n",
    "        question_id = question[\"Question_ID\"]\n",
    "        if int(question_id) < 10000:\n",
    "            question[\"answer\"] = full_data[f\"question {question_id}\"][\"answer\"]\n",
    "            aux.append(question)\n",
    "    test_data_json_with_context = aux\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the torch device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "answer_results = []\n",
    "\n",
    "if PUBLIC_TEST:\n",
    "    correct_answers = 0\n",
    "    total_answers = 0\n",
    "\n",
    "failed = 0\n",
    "\n",
    "pb = tqdm(test_data_json_with_context, desc=\"Generating test answers\", total=len(test_data_json_with_context), unit=\"Question\")\n",
    "\n",
    "# Iterate over the rows of the DataFrame\n",
    "for question_iter in pb:\n",
    "    # Get the question and answer\n",
    "    question = str(question_iter[\"question\"])\n",
    "\n",
    "    try:\n",
    "        option_1 = str(question_iter[\"options\"][\"option 1\"])\n",
    "        option_1_exists = True\n",
    "    except KeyError:\n",
    "        option_1 = \"\"\n",
    "        option_1_exists = False\n",
    "    try:\n",
    "        option_2 = str(question_iter[\"options\"][\"option 2\"])\n",
    "        option_2_exists = True\n",
    "    except KeyError:\n",
    "        option_2 = \"\"\n",
    "        option_2_exists = False\n",
    "    try:\n",
    "        option_3 = str(question_iter[\"options\"][\"option 3\"])\n",
    "        option_3_exists = True\n",
    "    except KeyError:\n",
    "        option_3 = \"\"\n",
    "        option_3_exists = False\n",
    "    try:\n",
    "        option_4 = str(question_iter[\"options\"][\"option 4\"])\n",
    "        option_4_exists = True\n",
    "    except KeyError:\n",
    "        option_4 = \"\"\n",
    "        option_4_exists = False\n",
    "    try:\n",
    "        option_5 = str(question_iter[\"options\"][\"option 5\"])\n",
    "        option_5_exists = True\n",
    "    except KeyError:\n",
    "        option_5_exists = False\n",
    "        option_5 = \"\"\n",
    "\n",
    "    # Update the question and answer in the DataFrame\n",
    "    merged_question = (\n",
    "        (\n",
    "            question\n",
    "            + \"\\n\"\n",
    "            + (\"\\n1. \" + option_1 if option_1_exists else \"\")\n",
    "            + (\"\\n2. \" + option_2 if option_2_exists else \"\")\n",
    "            + (\"\\n3. \" + option_3 if option_3_exists else \"\")\n",
    "            + (\"\\n4. \" + option_4 if option_4_exists else \"\")\n",
    "            + (\"\\n5. \" + option_5 if option_5_exists else \"\")\n",
    "        )\n",
    "        + \"\\n\\n\"\n",
    "        + \"Choose the correct option from the above options\"\n",
    "    )\n",
    "\n",
    "    context = \"\"\n",
    "    for ret in question_iter[\"context\"]:\n",
    "        context += ret\n",
    "\n",
    "    full_context = (\n",
    "        f\"Considering the following context:\\n{str(context)}\\n\"\n",
    "        + (\n",
    "            f\"Terms and Definitions:\\n{question_iter['terms']}\\n\"\n",
    "            if question_iter[\"terms\"]\n",
    "            else \"\"\n",
    "        )\n",
    "        + (\n",
    "            f\"Abbreviations: {question_iter['abbreviations']}\\n\"\n",
    "            if question_iter[\"abbreviations\"]\n",
    "            else \"\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    full_question = (\n",
    "        f\"Please provide the answer to the the following multiple choice question:\\n{merged_question}\\n\"\n",
    "        + \"Write only the option number corresponding to the correct answer.\"\n",
    "    )\n",
    "\n",
    "    input_tensor = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"context\",\n",
    "                \"content\": full_context,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": full_question,\n",
    "            }\n",
    "        ],\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Generate the answer\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_tensor.to(device),\n",
    "            max_length=2048,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode the answer\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    # Extract the answer from the full answer\n",
    "    match = re.search(r\"The correct option number is option (\\d)\", response)\n",
    "    if match:\n",
    "        answer = match.group(1)\n",
    "    else:\n",
    "        failed += 1\n",
    "        pb.set_postfix({\"Failed\": failed})\n",
    "        answer = -1\n",
    "\n",
    "    try:\n",
    "        answer = int(answer)\n",
    "    except ValueError:\n",
    "        failed += 1\n",
    "        answer = -1\n",
    "        pb.set_postfix({\"Failed\": failed})\n",
    "\n",
    "    if PUBLIC_TEST:\n",
    "        total_answers += 1\n",
    "        if answer == question_iter[\"answer\"]:\n",
    "            correct_answers += 1\n",
    "        pb.set_postfix({\"Correct\": correct_answers, \"Total\": total_answers, \"Accuracy\": correct_answers/total_answers})\n",
    "        \n",
    "\n",
    "    \n",
    "    # Update the DataFrame\n",
    "    answer_results.append(\n",
    "        {\n",
    "            \"Question_ID\": question_iter[\"Question_ID\"],\n",
    "            \"Answer_ID\": answer,\n",
    "            \"Task\": \"Phi-2\",\n",
    "            \"Correct_Answer\": question_iter[\"answer\"] if PUBLIC_TEST else None\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "df = pd.DataFrame(answer_results)\n",
    "\n",
    "if not PUBLIC_TEST:\n",
    "    df.drop(columns=[\"Correct_Answer\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show results and save as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {correct_answers/total_answers}, Correct: {correct_answers}, Total: {total_answers}\")\n",
    "df\n",
    "df.to_csv(\"./Data/results/test_results.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
