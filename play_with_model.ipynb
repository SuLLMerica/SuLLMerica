{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is for playing with the model and getting responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 22:35:34,226 - INFO - Load pretrained SentenceTransformer: BAAI/bge-large-en\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f5b2a5d9094838b5b3c775da5791ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ujson as json\n",
    "import re\n",
    "import os\n",
    "import traceback\n",
    "from Source.query import Query\n",
    "from Source.maneger_dataset import get_embeddings_by_labels\n",
    "from Source.generate_question import generate_questions_training\n",
    "from Source.enhancement_query import EnhancementQuery\n",
    "from Source.get_RAG_context import Get_RAG_Context\n",
    "from tqdm import tqdm\n",
    "import sqlite3\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NUM_CLUSTERS = 18\n",
    "TOP_K_CLUSTERS = 8\n",
    "TOP_K_CHUNCKS = 5\n",
    "DATABASE_PATH=\"cluster_data_BisectingKMeans_18_250_chunksize.db\"\n",
    "PATH_TERMS_FILE = \"./Data/TermsAndDefinitions/terms_definitions.json\"\n",
    "PATH_ABBREVIATIONS_FILE = \"./Data/TermsAndDefinitions/abbreviations_definitions.json\"\n",
    "MODEL_NAME = \"claudiomello/Phi-2-TeleQnA-Finetune-Final\"\n",
    "\n",
    "\n",
    "# Create a class for enhancement\n",
    "enhacenment_query = EnhancementQuery(file_name_terms=PATH_TERMS_FILE, file_name_abbreviations=PATH_ABBREVIATIONS_FILE)\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n",
      "3GPP stands for Third Generation Partnership Project and is an organization that develops technical specifications for 3rd generation wireless communications. It is composed of representatives from the telecommunications industry, government, and other interested parties. Some of the main organizations within 3GPP include the Technical Committee (TC), the Management Committee (MC), and the Reference Points Committee (RPC).\n"
     ]
    }
   ],
   "source": [
    "question = \"What is 3GPP?\"\n",
    "\n",
    "terms, abreviations = enhacenment_query.define_TA_question(question)\n",
    "\n",
    "conn = sqlite3.connect(DATABASE_PATH)\n",
    "\n",
    "try:\n",
    "    context_array = Get_RAG_Context(question, conn, NUM_CLUSTERS, TOP_K_CLUSTERS, TOP_K_CHUNCKS)\n",
    "    context = \"\"\n",
    "    for ret in context_array:\n",
    "        context += ret\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(traceback.format_exc())\n",
    "\n",
    "\n",
    "\n",
    "full_context = (\n",
    "    f\"Considering the following context:\\n{str(context)}\\n\"\n",
    "    + (\n",
    "        f\"Terms and Definitions:\\n{terms}\\n\"\n",
    "        if terms\n",
    "        else \"\"\n",
    "    )\n",
    "    + (\n",
    "        f\"Abbreviations: \\n{abreviations}\\n\"\n",
    "        if abreviations\n",
    "        else \"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "input_tensor = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"context\",\n",
    "            \"content\": full_context,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question,\n",
    "        }\n",
    "    ],\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "# Generate the answer\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_tensor.to(device),\n",
    "        max_length=2048,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "# Decode the answer\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "try:\n",
    "    initial_index = response.find(\"<|im_start|>assistant\")\n",
    "    final_index = response[initial_index:].find(\"<|im_end|>\")\n",
    "    correction = len(\"<|im_start|>assistant\")\n",
    "\n",
    "    answer = response[initial_index + correction:final_index+initial_index]\n",
    "    print(f\"Answer: {answer}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(traceback.format_exc())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
